{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Connect\n",
    "\n",
    "**Kafka Connect** is a tool that helps you **move data in and out of Apache Kafka** without having to write code, only configuration required. \n",
    "\n",
    "You can think of it as a **bridge** that connects Kafka with:\n",
    "\n",
    "- Databases like **MySQL, PostgreSQL, MongoDB**\n",
    "- Cloud services like **S3**\n",
    "- File systems like **local files or HDFS**\n",
    "- Message queues, APIs, and more\n",
    "\n",
    "\n",
    "**Kafka Connect Modes: Standalone vs Distributed**\n",
    "\n",
    "Kafka Connect supports two modes of operation:\n",
    "\n",
    "- **Standalone** - Best for local development and simple use cases. All connectors run in a single process.\n",
    "- **Distributed** - Best for production. Connectors are distributed across a Kafka Connect **cluster** of workers. Uses Kafka to coordinate and persist configs.\n",
    "\n",
    "Start with **standalone mode** while you're learning or testing locally.\n",
    "\n",
    "**Connectors**\n",
    "\n",
    "Kafka Connect uses **connectors** which are like plugins. There are two types:\n",
    "\n",
    "- **Source Connector**: Pulls data *into* Kafka (e.g., from a MySQL table)\n",
    "- **Sink Connector**: Pushes data *out of* Kafka (e.g., to a JSON file)\n",
    "\n",
    "\n",
    "**Why Should You Use Kafka Connect?**\n",
    "\n",
    "- **No coding needed** — just configuration\n",
    "- **Scalable** — can run standalone or distributed\n",
    "- **Automatic** — it keeps syncing data for you\n",
    "- **Great for data pipelines** and **ETL workflows**\n",
    "\n",
    "\n",
    "#### How Kafka Connect Works\n",
    "\n",
    "`[ External System ] ←→ [ Kafka Connect ] ←→ [ Apache Kafka ]`\n",
    "\n",
    "You configure connectors using simple `.properties` files or REST API calls. Lets begin with configuring Kafka Connect standalone mode. \n",
    "\n",
    "#### Setting the `plugin.path`\n",
    "\n",
    "Kafka Connect loads connector JARs from a directory called the **plugin path**. You must set this in your Connect worker config file, especially in **standalone mode**.\n",
    "\n",
    "In your `connect-standalone.properties` file, open the file with your favourite text editor and uncomment the line `plugin.path`. Then add the `libs` folder from your `kafka` folder i.e. the `kafka/libs` folder path which contains JAR files to run the connectors. Add the file path to your `kafka/libs` folder to the end of your `plugin.path`. For example using kafka version 3.9.0 with the username admin the filepath would be:\n",
    "\n",
    "`plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,/home/admin/kafka_2.12-3.9.0/libs`\n",
    "\n",
    "### Configuring connectors\n",
    "\n",
    "Let's look at two examples of how to configure a connector sink between you're file system and Kafka and a connector source between a PostgreSQL database. To configure a connector you will need to create a Java `.properties` file which contains key value pairs defining the connectors configuration. \n",
    "\n",
    "#### Writing Kafka Data to a JSON File (Sink)\n",
    "\n",
    "Let’s say you're working on a user tracking system. You publish JSON events to a Kafka topic called `user-events-topic`. Now, you want to archive that data to a local JSON file for backup or analytics. Let’s say you have a Kafka topic called `json-test`, and you want to save that data to a local file in JSON format.\n",
    "\n",
    "**Create the Connector Configuration**\n",
    "\n",
    "Make a directory in your `~/kafka/config` folder to store your connector configs. Let's create a folder called `sink-configs`\n",
    "\n",
    "Create a file inside the folder called `json_file_sink.properties`, open the file so you can begin configuring the connector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "name=user-json-file-sink\n",
    "connector.class=FileStreamSink\n",
    "tasks.max=1\n",
    "# Topic or topics that will send their data to the file sink\n",
    "topics=user-events-topic \n",
    "# File where topic data will be saved, remember to create the folders if they don't already exist\n",
    "file=/home/ubuntu/topic_data/user_events.json\n",
    "\n",
    "# Save all keys as strings \n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "# Save all values as JSON entries \n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "value.converter.schemas.enable=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to start Kafka Connect so that it can apply the configuration. Run the following command from your Kafka folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "bin/connect-standalone.sh config/connect-standalone.properties config/sink-configs/json_file_sink.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Kafka-Python` we can now send some data to the topic and check if the data has arrived in the `user_events.json` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Create Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',  # Adjust if your Kafka is on a different host/port\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')  # Send JSON-encoded messages\n",
    ")\n",
    "\n",
    "# Example data to send\n",
    "messages = [\n",
    "    {\"user\": \"alice\", \"action\": \"login\"},\n",
    "    {\"user\": \"bob\", \"action\": \"logout\"},\n",
    "    {\"user\": \"charlie\", \"action\": \"purchase\"}\n",
    "]\n",
    "\n",
    "# Send messages to 'kafka-test' topic\n",
    "for msg in messages:\n",
    "    producer.send('user-events-topic ', value=msg)\n",
    "    print(f\"Sent: {msg}\")\n",
    "    time.sleep(0.5)  # Optional delay for clarity\n",
    "\n",
    "# Wait for all messages to be delivered\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now viewing the `user_events.json` file in our `topic_data` folder we see the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"user\": \"alice\", \"action\": \"login\"},\n",
    "{\"user\": \"bob\", \"action\": \"logout\"},\n",
    "{\"user\": \"charlie\", \"action\": \"purchase\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As more data is sent to the topic messages will continue to be processed and sent to the `user_events.json` file. \n",
    "\n",
    "### PostgreSQL Source Connector – Reading from a Database\n",
    "\n",
    "Let's first begin by creating the test database that we're going to have Kafka extract data from. Open your database management system of choice and begin by creating a new database and table with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE DATABASE user_data;\n",
    "\n",
    "CREATE TABLE users (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    name VARCHAR(100),\n",
    "    email VARCHAR(100)\n",
    ");\n",
    "\n",
    "INSERT INTO users (name, email) VALUES\n",
    "('Ada Lovelace', 'ada@techmail.com'),\n",
    "('Alan Turing', 'alan@enigma.org');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we again need to configure the connector, so that data is being pulled from the database and sent to a topic when the database is altered. Create a new folder in your kafka `config` folder called `sources-config`. Then create a file inside that folder called `postgres_source.properties` and open the file to begin configuring it. Mainly here you need to update the connection string on line `5` to ensure that the connector connects to your database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "name=pg-user-source\n",
    "connector.class=io.confluent.connect.jdbc.JdbcSourceConnector\n",
    "tasks.max=1\n",
    "# connection string for you database. Update the string with the connection detail to your postgreSQL database.\n",
    "connection.url=jdbc:postgresql://localhost:5432/user_data?user=postgres&password=yourpassword\n",
    "# Take to extract data from and send to topic \n",
    "table.whitelist=users\n",
    "# Only fetch new rows based on the id column\n",
    "mode=incrementing\n",
    "incrementing.column.name=id\n",
    "# Prefix of the topic you want the table data send to. In our case the table is called users so data will\n",
    "# be sent to the topic pg-users\n",
    "topic.prefix=pg-\n",
    "poll.interval.ms=5000\n",
    "\n",
    "# Save all keys as strings \n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "# Save all values as JSON entries \n",
    "value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "value.converter.schemas.enable=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other avaliable mode options for the JDBC connecter are as follows: \n",
    "\n",
    "- **bulk**: Pulls the **entire table** every time it polls.                                    \n",
    "- **incrementing**: Uses a **monotonically increasing column** (like `id`) to fetch **new rows** only. \n",
    "- **timestamp**: Uses a **timestamp column** (like `last_updated`) to fetch new/updated rows.       \n",
    "- **timestamp+incrementing**: Uses **both** timestamp and incrementing columns for more accurate sync.          \n",
    "\n",
    "Now we just need to start Kafka Connect from the Kafka folder to ensure that the configuration has been applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "bin/connect-standalone.sh config/connect-standalone.properties config/source-config/postgres_source.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka should then begin pull data from the database and storing the data in the `pg-users` topic. Let's check the results using the Kafka Python API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Connect to Kafka and subscribe to the topic\n",
    "consumer = KafkaConsumer(\n",
    "    'pg-users',  # Replace with your topic name\n",
    "    bootstrap_servers='localhost:9092',  # Kafka broker address\n",
    "    group_id='postgres-reader-group',  # Consumer group name\n",
    "    auto_offset_reset='earliest',  # Start from beginning if no offset is stored\n",
    "    enable_auto_commit=True,  # Let Kafka manage offsets\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))  # Parse JSON values\n",
    ")\n",
    "\n",
    "print(\"Listening for messages on topic 'pg-users'...\\n\")\n",
    "\n",
    "# Read and print messages continuously\n",
    "for message in consumer:\n",
    "    print(\"Received message:\")\n",
    "    print(message.value)  # This will be a Python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see the messages in the topic being printed out to your terminal as JSON entires. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go Next with Kafka Connect\n",
    "\n",
    "Now that you've seen how to use Kafka Connect with PostgreSQL and local files, you're ready to explore a wide range of connectors and build more powerful data pipelines.\n",
    "\n",
    "### Other Common Kafka Connectors\n",
    "\n",
    "Here are some popular connectors used in real-world projects:\n",
    "\n",
    "#### Source Connectors (pull data into Kafka)\n",
    "\n",
    "- **Debezium** (PostgreSQL, MySQL, MongoDB) – captures changes (CDC) in real time\n",
    "- **JDBC Source** – pulls data from any relational database using SQL queries\n",
    "- **Kafka Connect File Source** – reads data from CSV, JSON, or text files\n",
    "- **MQTT Source** – pulls IoT sensor data from MQTT brokers\n",
    "- **S3 Source** – reads files from AWS S3 buckets\n",
    "\n",
    "#### Sink Connectors (push data out of Kafka)\n",
    "\n",
    "- **JDBC Sink** – write Kafka data to PostgreSQL, MySQL, SQL Server, etc.\n",
    "- **Elasticsearch Sink** – index data into Elasticsearch for search and analytics\n",
    "- **MongoDB Sink** – send JSON records to MongoDB collections\n",
    "- **S3 Sink** – archive Kafka data to AWS S3 as JSON, Avro, or Parquet\n",
    "- **BigQuery Sink** – stream Kafka data into Google BigQuery\n",
    "- **Kafka Connect File Sink** – save Kafka data to disk as files\n",
    "\n",
    "\n",
    "### Tips for Connecting External Systems\n",
    "\n",
    "- Check connector compatibility  \n",
    "  Always ensure the connector version matches your Kafka Connect version and Kafka cluster.\n",
    "\n",
    "- Set the `plugin.path` correctly  \n",
    "  Each connector should be placed in its own subfolder. Make sure `plugin.path` is defined in your worker configuration that points to this path. \n",
    "\n",
    "- Understand schemas  \n",
    "  Some connectors (especially Avro or Protobuf) require a Schema Registry to track data structure definitions.\n",
    "\n",
    "- Use the REST API in distributed mode  \n",
    "  For production deployments, the Kafka Connect REST API is the preferred way to create, monitor, and delete connectors programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Kafka Connect is a framework for integrating Kafka with external systems using configurable source and sink connectors.\n",
    "- Use standalone mode for local testing and distributed mode for scalable, production-grade deployments.\n",
    "- The `plugin.path` in your worker config tells Kafka Connect where to find installed connectors.\n",
    "- Source connectors bring data into Kafka; sink connectors send data out to databases, files, cloud storage, and more.\n",
    "- Use converters (`key.converter` and `value.converter`) to define how Kafka data is serialized (e.g., String, JSON, Avro).\n",
    "- Kafka Connect supports built-in and community connectors for systems like PostgreSQL, MySQL, S3, Elasticsearch, and MongoDB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
